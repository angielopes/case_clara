{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92075dd4",
   "metadata": {},
   "source": [
    "# Data Ingestion Notebook\n",
    "\n",
    "This notebook demonstrates the ingestion process for the case study selection process. It covers the following steps:\n",
    "\n",
    "- Loading configuration files\n",
    "- Reading data files into pandas DataFrames\n",
    "- Connecting to a MySQL database\n",
    "- Uploading data to the database\n",
    "\n",
    "Each step is explained in detail to ensure clarity and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362c8a5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We start by importing all necessary libraries for data manipulation, environment variable management, and database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv # Used to securely load environment variables from a .env file.\n",
    "from sqlalchemy import create_engine # Provides tools for connecting to and interacting with SQL databases.\n",
    "from urllib.parse import quote_plus # Ensures that special characters in the database password are safely encoded for use in the connection string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e4467",
   "metadata": {},
   "source": [
    "## 2. Load Ingestion Configuration\n",
    "\n",
    "The ingestion configuration is stored in a JSON file. This file specifies which tables to ingest and the corresponding file paths for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c51443",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config/ingestion.json\", \"r\") as open_json:\n",
    "    ingestions = json.load(open_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958bc426",
   "metadata": {},
   "source": [
    "## 3. Read Data Files\n",
    "\n",
    "For each table specified in the configuration, we read the corresponding CSV file into a pandas DataFrame. All DataFrames are stored in a dictionary for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf57e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "for item in ingestions:\n",
    "    table = item[\"table\"]\n",
    "    path = item[\"path\"]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\", sep=\",\")\n",
    "        dfs[table] = df\n",
    "        print(f\"Table {table} read.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading table {table}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9ac94",
   "metadata": {},
   "source": [
    "## 4. Data Preview\n",
    "\n",
    "(Optional) You can preview any of the loaded DataFrames. Uncomment and modify the following line to inspect a specific table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deba0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = dfs[\"user_table\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a2b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a5e75",
   "metadata": {},
   "source": [
    "## 5. Load Database Credentials\n",
    "\n",
    "We use environment variables to securely load the MySQL database password. The password is URL-encoded to ensure compatibility with the connection string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec895cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "password = quote_plus(os.getenv(\"DB_PASSWORD\")) # The password should be stored in the .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcabd406",
   "metadata": {},
   "source": [
    "## 6. Create SQLAlchemy Engine\n",
    "\n",
    "An SQLAlchemy engine is created to manage the connection to the MySQL database. This engine will be used to upload the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4029c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f\"mysql+pymysql://root:{password}@localhost/case_clara\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcc3b40",
   "metadata": {},
   "source": [
    "## 7. Upload Data to Database\n",
    "\n",
    "Each DataFrame is uploaded to its corresponding table in the MySQL database. The `if_exists=\"append\"` parameter ensures that new data is added without overwriting existing records.\n",
    "\n",
    "You can also upload a single DataFrame by uncommenting and modifying the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_sql(\"user_table\", con=engine, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, df in dfs.items():\n",
    "    df.to_sql(table_name, con=engine, if_exists=\"append\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
